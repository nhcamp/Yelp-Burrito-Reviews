{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `TextBlob` not found.\n"
     ]
    }
   ],
   "source": [
    "sys.execut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named textblob",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-626546dca925>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mio\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mvaderSentiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvaderSentiment\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named textblob"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from io import open\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'encoding' is an invalid keyword argument for this function",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-1e335f0d0d90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msentiment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\nhcam\\Miniconda2\\envs\\py36\\lib\\site-packages\\vaderSentiment\\vaderSentiment.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, lexicon_file, emoji_lexicon)\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[0m_this_module_file_path_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetsourcefile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[0mlexicon_full_filepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_this_module_file_path_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlexicon_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlexicon_full_filepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlexicon_full_filepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlexicon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_lex_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'encoding' is an invalid keyword argument for this function"
     ]
    }
   ],
   "source": [
    "sentiment = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'service_account' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-77dd88809620>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## Credential to authorize usage of Google's Cloud Natural Language API\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mservice_account\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCredentials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_service_account_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'google_service_account_cred.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'service_account' is not defined"
     ]
    }
   ],
   "source": [
    "## Credential to authorize usage of Google's Cloud Natural Language API\n",
    "cred = service_account.Credentials.from_service_account_file('google_service_account_cred.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This cell loads in the two JSON files, pulls out all the mexican restaurants from the business JSON and gathers all the\n",
    "## reviews that mention that restaurant. This takes a long time to compile, so I pickled the two DFs to avoid having to\n",
    "# reacess the massive JSON file.\n",
    "\n",
    "business=pd.read_json('business.json',lines='True')\n",
    "mex = business.categories.str.contains('Mexican', na=False)\n",
    "tex_mex = business.categories.str.contains('Tex-Mex', na=False)\n",
    "taqs = business.loc[mex | tex_mex]\n",
    "taqs_dict = {}\n",
    "taqs_id = []\n",
    "for index,data in taqs.iterrows():\n",
    "    taqs_dict[data['business_id']] = data['name']\n",
    "    taqs_id.append(data['business_id'])\n",
    "reviews = []\n",
    "with open('review.json') as fp:\n",
    "    for line in fp:\n",
    "        comment = json.loads(line) \n",
    "        reviews.append(comment)\n",
    "    fp.close()\n",
    "mexican_reviews = [rev for rev in reviews if rev[\"business_id\"] in taqs]  \n",
    "reviews_df = pd.DataFrame(mexican_reviews)\n",
    "reviews_df['Restaurant Name']=reviews_df['business_id'].map(taq_dict)\n",
    "reviews_df['reviews_length'] = reviews_df['text'].apply(len)\n",
    "reviews_df.to_pickle('./mexican_reviews.pkl')   \n",
    "business.to_pickle('./business_info.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions needed for throughout analysis\n",
    "\n",
    "def remove_stopwords_punc(i1):\n",
    "    ## Using nltk library's stop words and string.punctuation, this removes them from a tokenized list. Use with df.apply()\n",
    "    \n",
    "    stop_words = list(stopwords.words('english'))+list(string.punctuation)\n",
    "    minus_stops = [w for w in i1 if w not in stop_words]\n",
    "    return minus_stops\n",
    "\n",
    "def tokenize_and_clean(df):\n",
    "    ## Using nltk word tokenize in df.apply() and cleaning using function above.\n",
    "    \n",
    "    df['tokenized_text'] = df['text'].apply(word_tokenize)\n",
    "    df['tokenized_text_cleaned'] = df['tokenized_text'].apply(remove_stopwords_punc)\n",
    "    return df\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "def google_sentiment_analysis_magnitude(text):\n",
    "    client = language.LanguageServiceClient(credentials=cred)\n",
    "    text = text\n",
    "    document = types.Document(content=text,type=enums.Document.Type.PLAIN_TEXT)\n",
    "    sentiment = client.analyze_sentiment(document=document).document_sentiment\n",
    "    magnitude = sentiment.magnitude\n",
    "    return magnitude\n",
    "\n",
    "def google_sentiment_analysis_score(text):\n",
    "    client = language.LanguageServiceClient(credentials=cred)\n",
    "    text = text\n",
    "    document = types.Document(content=text,type=enums.Document.Type.PLAIN_TEXT)\n",
    "    sentiment = client.analyze_sentiment(document=document).document_sentiment\n",
    "    score = sentiment.score\n",
    "    return score\n",
    "\n",
    "def apply_sentiment_analysis(df):\n",
    "    tqdm.pandas()\n",
    "    df['Sentiment_Score'] = df['text'].progress_apply(google_sentiment_analysis_score)\n",
    "    df['Sentiment_Magnitude'] = df['text'].progress_apply(google_sentiment_analysis_magnitude)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192609it [00:18, 10553.34it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 5000.36it/s]\n"
     ]
    }
   ],
   "source": [
    "## This cell will load the pickled versions of the dataframe created above and perform manipulations...\n",
    "\n",
    "\n",
    "reviews_df = pd.read_pickle('mexican_reviews.pkl')\n",
    "business_df = pd.read_pickle('business_info.pkl')\n",
    "\n",
    "\n",
    "#Adding city and state data to the reviews\n",
    "city = {}\n",
    "state = {}\n",
    "for index,data in tqdm(business_df.iterrows()):\n",
    "    city[data['business_id']] = data['city']\n",
    "    state[data['business_id']] = data['state']\n",
    "reviews_df['city'] = reviews_df['business_id'].map(city)\n",
    "reviews_df['state'] = reviews_df['business_id'].map(state)\n",
    "\n",
    "\n",
    "#Adding region to reviews, dropping any row without a US State (there are british and canadian cities included)\n",
    "US_states = ['AL','AK','AZ','AR','CA','CO','CT','DE','FL','GA','HI','ID','IL',\n",
    "             'IN','IA','KS','KY','LA','ME','MD','MA','MI','MN','MS','MO','MT',\n",
    "             'NE','NV','NH','NJ','NM','NY','NC','ND','OH','OK','OR','PA','RI',\n",
    "             'SC','SD','TN','TX','UT','VT','VA','WA','WV','WI','WY']\n",
    "cali = ['CA']\n",
    "west = ['WA','OR','NV','ID','AK','HI','MT','UT','CO','WY','NM','AZ']\n",
    "midwest = ['ND','SD','NE','KS','OK','MN','IA','MO','WI','MI','IL','IN']\n",
    "south = ['LA','MS','AL','TN','NC','SC','GA','FL','TX','AR']\n",
    "noreast = ['KY','OH','WV','PA','MD','DE','NJ','NY','CT','RI','MA','VA','NH','ME','VT']\n",
    "states = [cali,west,midwest,south,noreast]\n",
    "regions = ['California','West','Midwest','South','Northeast']\n",
    "regions_dict={}\n",
    "i = 0\n",
    "for each in tqdm(states):\n",
    "    for indiv in each:\n",
    "        regions_dict[indiv] = regions[i]\n",
    "    i += 1\n",
    "reviews_df['region'] = reviews_df['state'].map(regions_dict)\n",
    "reviews_df = reviews_df.dropna(axis=0)\n",
    "\n",
    "#Mapping labels to stars\n",
    "stars_dict = {5:'Good',4:'Good',3:'Neutral/Bad',2:'Neutral/Bad',1:'Neutral/Bad'}\n",
    "reviews_df['Good/Neutral/Bad'] = reviews_df['stars'].map(stars_dict)\n",
    "\n",
    "\n",
    "#Set and sort index\n",
    "reviews_df.set_index(['business_id','Restaurant Name'],inplace=True)\n",
    "reviews_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nhcam\\Miniconda2\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "C:\\Users\\nhcam\\Miniconda2\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "burrito_mention = reviews_df.loc[reviews_df['text'].str.contains('burrito',case=False)]\n",
    "burrito_mention = tokenize_and_clean(burrito_mention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'encoding' is an invalid keyword argument for this function",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-28e136d8c53b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfirst_review\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreviews_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0manalyzer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\nhcam\\Miniconda2\\envs\\py36\\lib\\site-packages\\vaderSentiment\\vaderSentiment.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, lexicon_file, emoji_lexicon)\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[0m_this_module_file_path_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetsourcefile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[0mlexicon_full_filepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_this_module_file_path_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlexicon_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlexicon_full_filepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlexicon_full_filepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlexicon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_lex_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'encoding' is an invalid keyword argument for this function"
     ]
    }
   ],
   "source": [
    "first_review = reviews_df.iloc[1]['text']\n",
    "analyzer = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=dummy,preprocessor=dummy)\n",
    "text = list(burrito_mention['tokenized_text_cleaned'])\n",
    "bag_of_words = vectorizer.fit(text)\n",
    "X = bag_of_words.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bag_of_words.transform(text)\n",
    "y = burrito_mention['Good/Neutral/Bad']\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Good       0.89      0.93      0.91      8399\n",
      " Neutral/Bad       0.86      0.78      0.82      4542\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     12941\n",
      "   macro avg       0.87      0.85      0.86     12941\n",
      "weighted avg       0.88      0.88      0.88     12941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train,y_train)\n",
    "y_pred = mnb.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51764\n"
     ]
    }
   ],
   "source": [
    "print(len(burrito_mention))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
