{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DO NOT RUN THE CELL BELOW (it will take forever to compile). Details on functionality included in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### DONT RUN THIS CELL #################################################################\n",
    "\n",
    "### This cell loads in the two JSON files, pulls out all the mexican restaurants from the business JSON and gathers all the\n",
    "## reviews that mention that restaurant. This takes a long time to compile, so I pickled the two DFs to avoid having to\n",
    "# reacess the massive JSON file.\n",
    "\n",
    "business=pd.read_json('business.json',lines='True')\n",
    "mex = business.categories.str.contains('Mexican', na=False)\n",
    "tex_mex = business.categories.str.contains('Tex-Mex', na=False)\n",
    "taqs = business.loc[mex | tex_mex]\n",
    "taqs_dict = {}\n",
    "taqs_id = []\n",
    "for index,data in taqs.iterrows():\n",
    "    taqs_dict[data['business_id']] = data['name']\n",
    "    taqs_id.append(data['business_id'])\n",
    "reviews = []\n",
    "with open('review.json') as fp:\n",
    "    for line in fp:\n",
    "        comment = json.loads(line) \n",
    "        reviews.append(comment)\n",
    "    fp.close()\n",
    "mexican_reviews = [rev for rev in reviews if rev[\"business_id\"] in taqs]  \n",
    "reviews_df = pd.DataFrame(mexican_reviews)\n",
    "reviews_df['Restaurant Name']=reviews_df['business_id'].map(taq_dict)\n",
    "reviews_df['reviews_length'] = reviews_df['text'].apply(len)\n",
    "\n",
    "reviews_df.to_pickle('./mexican_reviews.pkl')   \n",
    "business.to_pickle('./business_info.pkl')\n",
    "\n",
    "## Here on will load the pickled versions of the dataframe created above, perform manipulations and repickle...\n",
    "reviews_path = 'C:/Users/nhcam/Desktop/Springboard/Yelp Burrito Reviews Project/Yelp_Project_Data/mexican_reviews.pkl'\n",
    "business_path = 'C:/Users/nhcam/Desktop/Springboard/Yelp Burrito Reviews Project/Yelp_Project_Data/business_info.pkl'\n",
    "\n",
    "reviews_df = pd.read_pickle(reviews_path)\n",
    "business_df = pd.read_pickle(business_path)\n",
    "\n",
    "\n",
    "#Adding city and state data to the reviews\n",
    "city = {}\n",
    "state = {}\n",
    "for index,data in tqdm(business_df.iterrows()):\n",
    "    city[data['business_id']] = data['city']\n",
    "    state[data['business_id']] = data['state']\n",
    "reviews_df['city'] = reviews_df['business_id'].map(city)\n",
    "reviews_df['state'] = reviews_df['business_id'].map(state)\n",
    "\n",
    "\n",
    "#Adding region to reviews, dropping any row without a US State (there are british and canadian cities included)\n",
    "US_states = ['AL','AK','AZ','AR','CA','CO','CT','DE','FL','GA','HI','ID','IL',\n",
    "             'IN','IA','KS','KY','LA','ME','MD','MA','MI','MN','MS','MO','MT',\n",
    "             'NE','NV','NH','NJ','NM','NY','NC','ND','OH','OK','OR','PA','RI',\n",
    "             'SC','SD','TN','TX','UT','VT','VA','WA','WV','WI','WY']\n",
    "cali = ['CA']\n",
    "west = ['WA','OR','NV','ID','AK','HI','MT','UT','CO','WY','NM','AZ']\n",
    "midwest = ['ND','SD','NE','KS','OK','MN','IA','MO','WI','MI','IL','IN']\n",
    "south = ['LA','MS','AL','TN','NC','SC','GA','FL','TX','AR']\n",
    "noreast = ['KY','OH','WV','PA','MD','DE','NJ','NY','CT','RI','MA','VA','NH','ME','VT']\n",
    "states = [cali,west,midwest,south,noreast]\n",
    "regions = ['California','West','Midwest','South','Northeast']\n",
    "regions_dict={}\n",
    "i = 0\n",
    "for each in tqdm(states):\n",
    "    for indiv in each:\n",
    "        regions_dict[indiv] = regions[i]\n",
    "    i += 1\n",
    "reviews_df['region'] = reviews_df['state'].map(regions_dict)\n",
    "reviews_df = reviews_df.dropna(axis=0)\n",
    "\n",
    "#Mapping labels to stars\n",
    "stars_dict = {5:'Good',4:'Good',3:'Neutral/Bad',2:'Neutral/Bad',1:'Neutral/Bad'}\n",
    "reviews_df['Good/Neutral/Bad'] = reviews_df['stars'].map(stars_dict)\n",
    "\n",
    "#Word tokenize review text and remove punctuation\n",
    "reviews_df = tokenize_and_clean(reviews_df)\n",
    "\n",
    "#Pull out reviews that mention burritos into new df and drop those rows from the reviews df\n",
    "burrito_mention = reviews_df.loc[reviews_df['text'].str.contains('burrito',case=False, regex = False)]\n",
    "reviews_df = reviews_df.loc[~reviews_df['text'].str.contains('burrito',case=False, regex = False)]\n",
    "\n",
    "#Find sentiment of burrito mentions df\n",
    "burrito_mention = find_burrito_sentences_get_sentiment(burrito_mention)\n",
    "\n",
    "#Set and sort index\n",
    "reviews_df.set_index(['business_id','Restaurant Name'],inplace=True)\n",
    "reviews_df.sort_index(inplace=True)\n",
    "burrito_mention.set_index(['business_id', 'Restaurant Name'],inplace=True)\n",
    "burrito_mention.sort_index(inplace=True)\n",
    "\n",
    "#Pickle dataframes\n",
    "reviews_df.to_pickle('./reviews_df.pkl')\n",
    "burrito_mention.to_pickle('./burrito_mentions.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions needed for throughout analysis\n",
    "\n",
    "def remove_stopwords_punc(i1):\n",
    "    ## Using nltk library's stop words and string.punctuation, this removes them from a tokenized list. Use with df.apply()\n",
    "    \n",
    "    stop_words = list(stopwords.words('english'))+list(string.punctuation)\n",
    "    minus_stops = [w for w in i1 if w not in stop_words]\n",
    "    return minus_stops\n",
    "\n",
    "def tokenize_and_clean(df):\n",
    "    ## Using nltk word tokenize in df.apply() and cleaning using function above.\n",
    "    \n",
    "    df['tokenized_text'] = df['text'].apply(word_tokenize)\n",
    "    df['tokenized_text_cleaned'] = df['tokenized_text'].apply(remove_stopwords_punc)\n",
    "    return df\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "def split_sentences_return_burrito(st):\n",
    "    if '.' in st:\n",
    "        sentences = re.split(r'[.?!]\\s*', st)\n",
    "        sentences_lower = [sentence.lower() for sentence in sentences]\n",
    "        burrito_sentence = [sentence for sentence in sentences_lower if 'burrito' in sentence]\n",
    "    else:\n",
    "        burrito_sentence = 'Punctuation lacking'\n",
    "    return burrito_sentence \n",
    "\n",
    "def apply_sentiment_intensity_analysis(sentence):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    polarity_dict = analyzer.polarity_scores(sentence)\n",
    "    return polarity_dict\n",
    "\n",
    "def find_burrito_sentences_get_sentiment(df):\n",
    "    df.reset_index(inplace=True)\n",
    "    df['burrito_sentences'] = df['text'].apply(split_sentences_return_burrito)\n",
    "    polarities_list = []\n",
    "    for indexes, data in df.iterrows():\n",
    "        polarity_dict = apply_sentiment_intensity_analysis(str(data['burrito_sentences']))\n",
    "        polarities_list.append(polarity_dict)\n",
    "    polarities_df = pd.DataFrame(polarities_list)\n",
    "    df = pd.concat([df, polarities_df],axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the two preprocessed dataframes from their pickled files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews_path = 'C:/Users/nhcam/Desktop/Springboard/Yelp Burrito Reviews Project/Yelp_Project_Data/reviews_df.pkl'\n",
    "burritos_reviews_path = 'C:/Users/nhcam/Desktop/Springboard/Yelp Burrito Reviews Project/Yelp_Project_Data/burrito_mentions.pkl'\n",
    "reviews_df = pd.read_pickle(all_reviews_path)\n",
    "burritos_df = pd.read_pickle(burritos_reviews_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=dummy,preprocessor=dummy)\n",
    "text = list(reviews_df['tokenized_text_cleaned'])\n",
    "bag_of_words = vectorizer.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bag_of_words.transform(text)\n",
    "y = reviews_df['Good/Neutral/Bad']\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Good       0.90      0.93      0.91     53263\n",
      " Neutral/Bad       0.86      0.80      0.83     28970\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     82233\n",
      "   macro avg       0.88      0.86      0.87     82233\n",
      "weighted avg       0.88      0.88      0.88     82233\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train,y_train)\n",
    "y_pred = mnb.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
